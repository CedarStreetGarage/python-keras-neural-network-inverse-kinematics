{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperbolic tangent activation and dense network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Import some libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import sin\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Define a model*\n",
    "\n",
    "In this case we are using a bipolar function that is continuous, monotone and bounded, which is to say it is a universal approximator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 5,251\n",
      "Trainable params: 5,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(50, input_dim=1, activation='tanh', use_bias=True),\n",
    "    Dense(50, activation='tanh', use_bias=True),\n",
    "    Dense(50, activation='tanh', use_bias=True),\n",
    "    Dense(1, activation='tanh', use_bias=True)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Fit model with a generator*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 14s - loss: 8.0505e-04 - val_loss: 2.1342e-04\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 14s - loss: 1.8147e-04 - val_loss: 1.5821e-04\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 14s - loss: 1.4495e-04 - val_loss: 7.8119e-05\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 14s - loss: 7.0941e-05 - val_loss: 1.8047e-05\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 14s - loss: 3.4249e-05 - val_loss: 1.8572e-05\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 14s - loss: 2.4468e-05 - val_loss: 1.0710e-05\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 14s - loss: 2.2358e-05 - val_loss: 3.6598e-06\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 14s - loss: 1.9545e-05 - val_loss: 7.2778e-06\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 14s - loss: 1.3780e-05 - val_loss: 1.8593e-05\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 13s - loss: 1.6401e-05 - val_loss: 8.6092e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efd6db0e510>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_batch(batch_size):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(10):\n",
    "        x = np.random.uniform(-np.pi/2, np.pi/2)\n",
    "        y = sin(x)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return np.array([X, Y])\n",
    "\n",
    "def gen(batch_size):\n",
    "    while 1:\n",
    "        yield gen_batch(batch_size)\n",
    "        \n",
    "model.fit_generator(generator=gen(20), steps_per_epoch=10000, epochs=10, validation_data=gen(5), validation_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Validate against original function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((-1.5, -0.9940923452377319, -0.9974949866040544), 0.0034111864340358395)\n",
      "((-1.4, -0.984131932258606, -0.9854497299884601), 0.0013372551534106343)\n",
      "((-1.3, -0.9617215991020203, -0.963558185417193), 0.0019060460934982413)\n",
      "((-1.2, -0.9290411472320557, -0.9320390859672263), 0.0032165375683354575)\n",
      "((-1.1, -0.8879474401473999, -0.8912073600614354), 0.0036578691560747337)\n",
      "((-1.0, -0.8381936550140381, -0.8414709848078965), 0.003894762687042164)\n",
      "((-0.9, -0.7796767950057983, -0.7833269096274834), 0.004659759005880331)\n",
      "((-0.8, -0.7130698561668396, -0.7173560908995228), 0.00597504473309553)\n",
      "((-0.7, -0.6395702958106995, -0.644217687237691), 0.007214007809873821)\n",
      "((-0.6, -0.5603370666503906, -0.5646424733950354), 0.007625013964602334)\n",
      "((-0.5, -0.4759567975997925, -0.479425538604203), 0.007235202810658354)\n",
      "((-0.4, -0.3864668011665344, -0.3894183423086505), 0.0075793582927245005)\n",
      "((-0.3, -0.29267024993896484, -0.29552020666133955), 0.009643864135628118)\n",
      "((-0.2, -0.19765667617321014, -0.19866933079506122), 0.005097186454489463)\n",
      "((-0.1, -0.10075800865888596, -0.09983341664682815), 0.00926134798459967)\n",
      "((0.1, 0.09934858977794647, 0.09983341664682815), 0.004856358573771062)\n",
      "((0.2, 0.1953715831041336, 0.19866933079506122), 0.016599178533144736)\n",
      "((0.3, 0.2908143401145935, 0.29552020666133955), 0.01592400939316773)\n",
      "((0.4, 0.3853261172771454, 0.3894183423086505), 0.010508557473807089)\n",
      "((0.5, 0.475504070520401, 0.479425538604203), 0.008179514373011804)\n",
      "((0.6, 0.5605025291442871, 0.5646424733950354), 0.0073319745605674135)\n",
      "((0.7, 0.6402483582496643, 0.644217687237691), 0.006161471606044532)\n",
      "((0.8, 0.7141109704971313, 0.7173560908995228), 0.00452372321579127)\n",
      "((0.9, 0.7809010744094849, 0.7833269096274834), 0.0030968363121243645)\n",
      "((1.0, 0.8394343852996826, 0.8414709848078965), 0.0024202848879915128)\n",
      "((1.1, 0.8891008496284485, 0.8912073600614354), 0.0023636591520538133)\n",
      "((1.2, 0.9300790429115295, 0.9320390859672263), 0.0021029622954735925)\n",
      "((1.3, 0.962601363658905, 0.963558185417193), 0.0009930088008890223)\n",
      "((1.4, 0.9846521019935608, 0.9854497299884601), 0.000809405057027813)\n"
     ]
    }
   ],
   "source": [
    "x = [float(x-15)/10 for x in range(30)]\n",
    "y = model.predict(np.array(x))\n",
    "z = [sin(i) for i in x]\n",
    "\n",
    "q = zip(x,[i[0] for i in y.tolist()],z)\n",
    "for p in q:\n",
    "    if p[2] != 0:\n",
    "        print(p, abs(p[1] - p[2]) / abs(p[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results aren't that bad.  However, in general, the greater the slope, the worse the performance. The degree depends on the function being approximated and the activation function.  For example, sin^2 is better approximated by a sigmoid than tanh, which works since sigmoid is a universal approximator and is strictly positive.\n",
    "\n",
    "Since we are interested in areas near singularities for inverse kinematics, the problem with accuracy at higher gradients is a problem.  One solution is to use more neurons, but the number of neurons grows quickly with the size of the work area and the gradient of the manifold we are approximating.  Another option is to use a significantly different type of activation function having a geometric aspect, such as radial basis functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radial Basis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Import some libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.initializers import RandomUniform, Initializer, Orthogonal, Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Define the custom layer* \n",
    "(Idea taken from https://github.com/PetraVidnerova/rbf_keras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFLayer(Layer):\n",
    "   \n",
    "    def __init__(self, output_dim, initializer=RandomUniform(0.0, 1.0), betas=1.0, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(RBFLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.centers = self.add_weight(shape       = (self.output_dim, input_shape[1]),\n",
    "                                       initializer = RandomUniform(0.0, 1.0),\n",
    "                                       trainable   = True)\n",
    "        self.betas = self.add_weight(shape       = (self.output_dim),\n",
    "                                     initializer = Constant(value=1.0),\n",
    "                                     trainable   = True)\n",
    "        super(RBFLayer, self).build(input_shape)  \n",
    "\n",
    "    def call(self, x):\n",
    "        C = K.expand_dims(self.centers)\n",
    "        H = (C-x.T).T\n",
    "        return K.exp(-self.betas * K.sum(H**2, axis=1))\n",
    "   \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'output_dim': self.output_dim\n",
    "        }\n",
    "        base_config = super(RBFLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Define an initializer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitCentersRandom(Initializer):\n",
    "  \n",
    "    def __init__(self, X):\n",
    "        self.X = X \n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        print(shape)\n",
    "        print(self.X.shape)\n",
    "        assert shape[1] == self.X.shape[1]\n",
    "        idx = np.random.randint(self.X.shape[0], size=shape[0])\n",
    "        return self.X[idx,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Define basis and model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(zip([float(x-5)/10 for x in range(10)],[float(x-5)/10 for x in range(10)]))\n",
    "  \n",
    "model = Sequential([\n",
    "    RBFLayer(10, initializer=InitCentersRandom(X), betas=1.0, input_shape=(1,)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.fit_generator(generator=gen(20), steps_per_epoch=10000, epochs=10, validation_data=gen(5), validation_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Validate against original function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [float(x-15)/10 for x in range(30)]\n",
    "y = model.predict(np.array(x))\n",
    "z = [sin(i) for i in x]\n",
    "\n",
    "q = zip(x,[i[0] for i in y.tolist()],z)\n",
    "for p in q:\n",
    "    if p[2] != 0:\n",
    "        print(p, abs(p[1] - p[2]) / abs(p[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
